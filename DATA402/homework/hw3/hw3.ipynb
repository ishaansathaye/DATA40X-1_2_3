{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3 - Ishaan Sathaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Express this model in matrix form. For each matrix involved in the equation, state the dimensions, and what those dimensions represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Y = \\begin{bmatrix} 1,000,000 \\\\ 400,000 \\\\ 700,000 \\end{bmatrix}$\n",
    "- where the dimensions are 3x1 and 3 rows represents the number of observations and the 1 column represents the house price.\n",
    "\n",
    "- $X = \\begin{bmatrix} 1 & 4700 & 3 \\\\ 1 & 1050 & 1 \\\\ 1 & 2200 & 2 \\end{bmatrix}$\n",
    "- where the dimensions are 3x3 and 3 rows represents the number of observations and the 3 columns represent the intercept, square footage, and number of bedrooms.\n",
    "\n",
    "- $\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}$\n",
    "- where the dimensions are 3x1 and 3 rows represents the number of coefficients (including intercept) and the 1 column represents the coefficient values.\n",
    "\n",
    "- Matrix Form:\n",
    "    - $Y = X\\beta$\n",
    "    - $\\begin{bmatrix} 1,000,000 \\\\ 400,000 \\\\ 700,000 \\end{bmatrix} = \\begin{bmatrix} 1 & 4700 & 3 \\\\ 1 & 1050 & 1 \\\\ 1 & 2200 & 2 \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the hat matrix for this data. Explain why you got the results you did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 0.00000000e+00 4.44089210e-16]\n",
      " [7.10542736e-15 1.00000000e+00 5.32907052e-15]\n",
      " [1.06581410e-14 5.32907052e-15 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hat_matrix(X):\n",
    "    H = X @ np.linalg.inv(X.T @ X) @ X.T\n",
    "    return H\n",
    "\n",
    "X = np.array([[1, 4700, 3], [1, 1050, 1], [1, 2200, 2]])\n",
    "H = hat_matrix(X)\n",
    "print(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the hat matrix, I got a identity matrix with dimensions 3x3, 0s everywhere but the diagonal. The hat matrix should be a identity matrix because the model is a perfect fit. The model is a perfect fit because the number of observations is equal to the number of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Express this model in matrix form. For each matrix involved in the equation, state the dimensions, and what those dimensions represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Y = \\begin{bmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}$\n",
    "- where the dimensions are nx1 and n rows represents then number of strains in the dataset and the 1 column represents the rating of each strain.\n",
    "\n",
    "- $X = \\begin{bmatrix} 1 & X_{11} & X_{12} & \\dots & X_{163} \\\\ 1 & X_{21} & X_{22} & \\dots & X_{263} \\\\ \\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & X_{n1} & X_{n2} & \\dots & X {n63} \\end{bmatrix}$\n",
    "- where the dimensions are nx164 and n rows represents the number of strains in the dataset and the 64 columns represent the intercept and the 63 columns for the dummy variables representing the effects and flavors of the strains.\n",
    "\n",
    "- $\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{63} \\end{bmatrix}$\n",
    "- where the dimensions are 64x1 and 64 rows represent the intercept nad coefficients for each of the 63 dummy variables and the 1 column represents the coefficient values.\n",
    "\n",
    "- $\\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}$\n",
    "- where the dimensions are nx1 and n rows represents the residuals for each strain and the 1 column represents the residual values.\n",
    "\n",
    "- Matrix Form:\n",
    "    - $Y = X\\beta + \\epsilon$\n",
    "    - $\\begin{bmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} = \\begin{bmatrix} 1 & X_{11} & X_{12} & \\dots & X_{163} \\\\ 1 & X_{21} & X_{22} & \\dots & X_{263} \\\\ \\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & X_{n1} & X_{n2} & \\dots & X_{n63} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{63} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Give the *equation* for the hat matrix for this model using a OLS. Where would you expect this equation to \"break\"?\n",
    "    - $H = X(X^TX)^{-1}X^T$\n",
    "    - The equation would break if the matrix $X^TX$ is not invertible. This would happen if any of the columns are highly correlated or linearly dependent. $X^TX$ would be singular and so the inverse would not exist.\n",
    "    - The equation would also break if the number of observations is less than the number of coefficients. This would make the matrix $X^TX$ singular and the inverse would not exist. However from the csv file, I know that this is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Give the *equation* for the hat matrix for this model using a *Ridge penalty*.\n",
    "    - $H = X(X^TX + \\lambda I)^{-1}X^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Find the equation for the OLS hat matrix for this model predicting residuals. How does it compare to your hat matrix in A2?\n",
    "    - $H = X(X^TX)^{-1}X^T$\n",
    "    - The hat matrix for predicting residuals is identical to the hat matrix in A2 and this is because the same design matrix and OLS formula is used. But this hat matrix is used to essentially map the observed residuals R to the predicted residuals $\\hat{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Would you recommend this strategy? Why or why not? Justify your answer mathematically.\n",
    "    - I would not recommend this strategy because if you are predicting further from the design matrix, it would result in redundant predictions.\n",
    "    - $R = Y - H_1Y$, where $H_1$ is the hat matrix for predicting Y and it is orthogonal to the design matrix.\n",
    "    - So fitting a model to predict the residuals using the same design matrix is ineffective:\n",
    "        - $H_2R = X(X^TX)^{-1}X^TR = 0$\n",
    "    - This is because the residuals are orthogonal to the design matrix and so the predicted residuals would be 0.\n",
    "    - So if $\\hat{R} = 0$, then the predicted house prices will be the same as the first model's predictions:\n",
    "        - $\\hat{Y} = H_1Y - H_2R = H_1Y - 0 = H_1Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the data in, and drop all rows with missing data in the predictors. Decide if you want to standardize anything or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strain</th>\n",
       "      <th>Type</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Effects</th>\n",
       "      <th>Flavor</th>\n",
       "      <th>Creative</th>\n",
       "      <th>Energetic</th>\n",
       "      <th>Tingly</th>\n",
       "      <th>Euphoric</th>\n",
       "      <th>Relaxed</th>\n",
       "      <th>...</th>\n",
       "      <th>Ammonia</th>\n",
       "      <th>Minty</th>\n",
       "      <th>Tree</th>\n",
       "      <th>Fruit</th>\n",
       "      <th>Butter</th>\n",
       "      <th>Pineapple</th>\n",
       "      <th>Tar</th>\n",
       "      <th>Rose</th>\n",
       "      <th>Plum</th>\n",
       "      <th>Pear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100-Og</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n",
       "      <td>Earthy,Sweet,Citrus</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98-White-Widow</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n",
       "      <td>Flowery,Violet,Diesel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>sativa</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n",
       "      <td>Spicy/Herbal,Sage,Woody</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13-Dawgs</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n",
       "      <td>Apricot,Citrus,Grapefruit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24K-Gold</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n",
       "      <td>Citrus,Earthy,Orange</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Strain    Type  Rating                                     Effects  \\\n",
       "0          100-Og  hybrid     4.0  Creative,Energetic,Tingly,Euphoric,Relaxed   \n",
       "1  98-White-Widow  hybrid     4.7    Relaxed,Aroused,Creative,Happy,Energetic   \n",
       "2            1024  sativa     4.4   Uplifted,Happy,Relaxed,Energetic,Creative   \n",
       "3        13-Dawgs  hybrid     4.2     Tingly,Creative,Hungry,Relaxed,Uplifted   \n",
       "4        24K-Gold  hybrid     4.6   Happy,Relaxed,Euphoric,Uplifted,Talkative   \n",
       "\n",
       "                      Flavor  Creative  Energetic  Tingly  Euphoric  Relaxed  \\\n",
       "0        Earthy,Sweet,Citrus       1.0        1.0     1.0       1.0      1.0   \n",
       "1      Flowery,Violet,Diesel       1.0        1.0     0.0       0.0      1.0   \n",
       "2    Spicy/Herbal,Sage,Woody       1.0        1.0     0.0       0.0      1.0   \n",
       "3  Apricot,Citrus,Grapefruit       1.0        0.0     1.0       0.0      1.0   \n",
       "4       Citrus,Earthy,Orange       0.0        0.0     0.0       1.0      1.0   \n",
       "\n",
       "   ...  Ammonia  Minty  Tree  Fruit  Butter  Pineapple  Tar  Rose  Plum  Pear  \n",
       "0  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "1  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "2  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "3  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "4  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./cannabis_full.csv\")\n",
    "predictors = df.drop(columns=['Strain', 'Type', 'Effects', 'Flavor', 'Rating'])\n",
    "df_clean = df.dropna(subset=predictors.columns)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function to compute beta estimates for Ordinary Least Squares regression with multiple predictors. Run this function on the Cannabis data with three predictors: Type, Relaxed, and Energetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.74369181 0.0246247  0.06176504 0.61677751 0.3575477 ]\n"
     ]
    }
   ],
   "source": [
    "def compute_betas(X, y):\n",
    "    betas = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return betas\n",
    "\n",
    "df_clean = pd.get_dummies(df_clean, columns=['Type'], drop_first=True)\n",
    "X = df_clean[['Type_indica', 'Type_sativa', 'Relaxed', 'Energetic']].values\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "y = df_clean['Rating'].values\n",
    "betas = compute_betas(X, y)\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function to compute beta estimates for Ridge regression, with $\\lambda$ as a user input. Use the following loss function:\n",
    "\n",
    "- $l(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_betas_ridge(X, y, lam):\n",
    "    n, p = X.shape\n",
    "    betas = np.linalg.inv(X.T @ X + lam * np.eye(p)) @ X.T @ y\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run your Ridge function on the Cannabis data with three predictors: Type, Relaxed, and Energetic with $\\lambda = 1000$. How do these estimates compare to your OLS estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.09146146 0.42927685 0.37862156 1.29022622 0.59328568]\n"
     ]
    }
   ],
   "source": [
    "X = df_clean[['Type_indica', 'Type_sativa', 'Relaxed', 'Energetic']].values\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "y = df_clean['Rating'].values\n",
    "betas_ridge = compute_betas_ridge(X, y, 1000)\n",
    "print(betas_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The OLS estimates are higher than the Ridge estimates. This is because the Ridge estimates are penalized by the $\\lambda$ term and so the coefficients are shrunk towards 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run your Ridge function on the Cannabis data with all the predictors with $\\lambda = 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.80029634e-01 4.20427905e-01 2.70827248e-01 7.17088330e-01\n",
      " 8.11775732e-01 2.06998827e-01 8.43333169e-01 7.00186100e-01\n",
      " 2.95768601e-01 3.05102280e-01 2.22007493e-01 3.94345823e-01\n",
      " 3.74779752e-01 3.56266939e-03 3.56266939e-03 4.21006427e-01\n",
      " 4.33017084e-01 2.75309045e-01 1.66117427e-01 8.59336419e-03\n",
      " 1.57083178e-01 1.67099289e-01 3.22719662e-02 1.11911001e-01\n",
      " 8.59998596e-03 2.56604438e-02 5.21142541e-02 2.10353024e-01\n",
      " 1.02429552e-01 1.79572463e-01 1.15369591e-01 1.89508957e-01\n",
      " 5.91956879e-02 2.29877832e-02 9.22359824e-02 4.08288775e-02\n",
      " 2.91941620e-02 2.18929490e-02 1.24655405e-01 8.58042452e-03\n",
      " 2.34630359e-02 2.09105933e-02 9.19884385e-03 1.31976747e-02\n",
      " 4.98400069e-03 1.02501134e-01 3.16569077e-02 8.76123475e-02\n",
      " 5.27809201e-02 1.16266605e-02 2.76326947e-02 2.96246338e-02\n",
      " 4.54533066e-02 2.30035231e-02 2.76335381e-02 4.15639678e-02\n",
      " 2.41932056e-02 2.41932056e-02 1.32495822e-02 2.34543245e-02\n",
      " 6.22272058e-03 1.25641352e-02 5.14925575e-04 3.23613676e-03\n",
      " 2.94807678e-01 2.42561359e-01]\n"
     ]
    }
   ],
   "source": [
    "predictors = df_clean.drop(columns=['Strain', 'Effects', 'Flavor', 'Rating'])\n",
    "X = predictors.values\n",
    "betas_ridge = compute_betas_ridge(X, y, 1000)\n",
    "print(betas_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function to perform *tuning* on $\\lambda$, using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lambda_split(train, test, lam_values, metric):\n",
    "    X_train = train.drop(columns=['Rating']).values\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    y_train = train['Rating'].values\n",
    "    X_test = test.drop(columns=['Rating']).values\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    y_test = test['Rating'].values\n",
    "    metrics = []\n",
    "    for lam in lam_values:\n",
    "        betas = compute_betas_ridge(X_train, y_train, lam)\n",
    "        y_pred = X_test @ betas\n",
    "        if metric == 'r-sq':\n",
    "            y_bar = np.mean(y_test)\n",
    "            ss_tot = np.sum((y_test - y_bar) ** 2)\n",
    "            ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "            r2 = 1 - ss_res / ss_tot\n",
    "            metrics.append(r2)\n",
    "        elif metric == 'mse':\n",
    "            mse = np.mean((y_test - y_pred) ** 2)\n",
    "            metrics.append(mse)\n",
    "        elif metric == 'mae':\n",
    "            mae = np.mean(np.abs(y_test - y_pred))\n",
    "            metrics.append(mae)\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metrics})\n",
    "\n",
    "def tune_lambda(df, lam_values, metric, k):\n",
    "    n = df.shape[0]\n",
    "    fold_size = n // k\n",
    "    metrics = []\n",
    "    for lam in lam_values:\n",
    "        metric_values = []\n",
    "        for i in range(k):\n",
    "            test = df.iloc[i * fold_size:(i + 1) * fold_size]\n",
    "            train = df.drop(test.index)\n",
    "            df_metric = tune_lambda_split(train, test, [lam], metric)\n",
    "            metric_values.append(df_metric[metric].values[0])\n",
    "        metrics.append(np.mean(metric_values))\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use your tuning function to choose a best $\\lambda$ for Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.297508346206442\n",
      "16.297508346206442\n",
      "11.497569953977356\n"
     ]
    }
   ],
   "source": [
    "predictors = df_clean.drop(columns=['Strain', 'Effects', 'Flavor'])\n",
    "lam_values = np.logspace(0, 5, num=100)\n",
    "folds = 5\n",
    "df_tune_rsq = tune_lambda(predictors, lam_values, 'r-sq', folds)\n",
    "best_lambda_rsq = df_tune_rsq.loc[df_tune_rsq['r-sq'].idxmax()]['lambda']\n",
    "print(best_lambda_rsq)\n",
    "\n",
    "df_tune_mse = tune_lambda(predictors, lam_values, 'mse', folds)\n",
    "best_lambda_mse = df_tune_mse.loc[df_tune_mse['mse'].idxmin()]['lambda']\n",
    "print(best_lambda_mse)\n",
    "\n",
    "df_tune_mae = tune_lambda(predictors, lam_values, 'mae', folds)\n",
    "best_lambda_mae = df_tune_mae.loc[df_tune_mae['mae'].idxmin()]['lambda']\n",
    "print(best_lambda_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seems that $\\lambda = 16.29$ is the best $\\lambda$ for Ridge regression as it got the highest R-sq and the lowest mse, and $\\lambda = 11.49$ for the lowest mae.\n",
    "<!-- - Seems that $\\lambda = 1$ is the best $\\lambda$ for Ridge regression. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The documentation for the linear_model.Ridge function in scikit-learn here. The argument alpha is the penalty parameter here - but the loss function is parametrized differently than in B3. What would you have to plug in for alpha to get the same results as you got in B5?\n",
    "    - The loss function in scikit-learn is parametrized as:\n",
    "        - $||y - Xw||_2^2 + \\alpha ||w||_2^2$\n",
    "    - While the loss function in B3 is parametrized as:\n",
    "        - $\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n",
    "    - To get the same results as in B5, you would just plug in 1000*n for alpha, where n is the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In tidymodels, the penalty argument of linear_reg() is the penalty parameter. If we use the glmnet package as our engine, penalty gets passed to the argument lambda of the function glmnet. Read the documentation for the glmnet() function - scroll down to the “Details” section. What would you have to plug in for penalty to get the same results as you got in B5?\n",
    "    - Plug in 2 times the $\\lambda$ times the number of observations for penalty to get the same results as in B5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fit your final model from B7, and interpret the three largest coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest 3 coefficients:\n",
      "Intercept: 1.640762409963628\n",
      "Relaxed: 0.6013594958352564\n",
      "Creative: 0.48773022896942014\n"
     ]
    }
   ],
   "source": [
    "predictors = df_clean.drop(columns=['Strain', 'Effects', 'Flavor', 'Rating'])\n",
    "X = predictors.values\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "betas_ridge = compute_betas_ridge(X, y, best_lambda_rsq)\n",
    "\n",
    "predictor_names = predictors.columns\n",
    "coefficients_with_names = list(zip(betas_ridge, ['Intercept'] + list(predictor_names)))\n",
    "largest_coefficients = sorted(coefficients_with_names, key=lambda x: abs(x[0]), reverse=True)[:3]\n",
    "print(\"Largest 3 coefficients:\")\n",
    "for coef, name in largest_coefficients:\n",
    "    print(f\"{name}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpretation:\n",
    "    - For every 1 unit increase in the Happy rating, the rating of the strain increases by 0.617.\n",
    "    - For every 1 unit increase in the Relaxed rating, the rating of the strain increases by 0.579.\n",
    "    - The intercept is the rating of the strain when all the predictors are 0, which is not interpretable in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use built-in functions from scikit-learn or tidymodels to fit a LASSO Regression, using all the predictors. Use the same value of that you found to be “best” in B5. Interpret the largets three coefficients - are they the same as in C1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest 3 coefficients:\n",
      "Creative: 0.0\n",
      "Energetic: 0.0\n",
      "Tingly: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "predictors = df_clean.drop(columns=['Strain', 'Effects', 'Flavor', 'Rating'])\n",
    "X = predictors.values\n",
    "y = df_clean['Rating'].values\n",
    "\n",
    "lasso = Lasso(alpha=best_lambda_rsq)\n",
    "lasso.fit(X, y)\n",
    "coefficients_with_names = list(zip(lasso.coef_, predictors.columns))\n",
    "largest_coefficients = sorted(coefficients_with_names, key=lambda x: abs(x[0]), reverse=True)[:3]\n",
    "print(\"Largest 3 coefficients:\")\n",
    "for coef, name in largest_coefficients:\n",
    "    print(f\"{name}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpretation:\n",
    "    - The coefficients are 0 because LASSO regression shrinks the coefficients to 0 and it would seem that the lambda value is too high. The coefficients are not same as in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In which of these model fitting approaches (OLS, Ridge, LASSO) was it important to standardize our predictors before fitting the model? Why?\n",
    "- It is important to standardize the predictors before fitting the model in Ridge and LASSO regression because the penalty term is dependent on the scale of the predictors. If the predictors are not standardized, then the penalty term would be biased towards the predictors with larger scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Which of the three model fitting approaches do you prefer in this data scenario? Why? (There is no single right answer to this question; I want you to justify your choice in a reasonable way.)\n",
    "- I prefer Ridge regression in this data scenario because it is a good compromise between OLS and LASSO regression. Ridge regression is able to shrink the coefficients towards 0 and so it is able to handle multicollinearity and overfitting. LASSO regression is too harsh in shrinking the coefficients to 0 and so it is not able to handle multicollinearity as well as Ridge regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
