{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3 - Ishaan Sathaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Express this model in matrix form. For each matrix involved in the equation, state the dimensions, and what those dimensions represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Y = \\begin{bmatrix} 1,000,000 \\\\ 400,000 \\\\ 700,000 \\end{bmatrix}$\n",
    "- where the dimensions are 3x1 and 3 rows represents the number of observations and the 1 column represents the house price.\n",
    "\n",
    "- $X = \\begin{bmatrix} 1 & 4700 & 3 \\\\ 1 & 1050 & 1 \\\\ 1 & 2200 & 2 \\end{bmatrix}$\n",
    "- where the dimensions are 3x3 and 3 rows represents the number of observations and the 3 columns represent the intercept, square footage, and number of bedrooms.\n",
    "\n",
    "- $\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}$\n",
    "- where the dimensions are 3x1 and 3 rows represents the number of coefficients (including intercept) and the 1 column represents the coefficient values.\n",
    "\n",
    "- Matrix Form:\n",
    "    - $Y = X\\beta$\n",
    "    - $\\begin{bmatrix} 1,000,000 \\\\ 400,000 \\\\ 700,000 \\end{bmatrix} = \\begin{bmatrix} 1 & 4700 & 3 \\\\ 1 & 1050 & 1 \\\\ 1 & 2200 & 2 \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the hat matrix for this data. Explain why you got the results you did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 0.00000000e+00 4.44089210e-16]\n",
      " [7.10542736e-15 1.00000000e+00 5.32907052e-15]\n",
      " [1.06581410e-14 5.32907052e-15 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hat_matrix(X):\n",
    "    H = X @ np.linalg.inv(X.T @ X) @ X.T\n",
    "    return H\n",
    "\n",
    "X = np.array([[1, 4700, 3], [1, 1050, 1], [1, 2200, 2]])\n",
    "H = hat_matrix(X)\n",
    "print(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the hat matrix, I got a identity matrix with dimensions 3x3, 0s everywhere but the diagonal. The hat matrix should be a identity matrix because the model is a perfect fit. The model is a perfect fit because the number of observations is equal to the number of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Express this model in matrix form. For each matrix involved in the equation, state the dimensions, and what those dimensions represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Y = \\begin{bmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}$\n",
    "- where the dimensions are nx1 and n rows represents then number of strains in the dataset and the 1 column represents the rating of each strain.\n",
    "\n",
    "- $X = \\begin{bmatrix} 1 & X_{11} & X_{12} & \\dots & X_{163} \\\\ 1 & X_{21} & X_{22} & \\dots & X_{263} \\\\ \\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & X_{n1} & X_{n2} & \\dots & X {n63} \\end{bmatrix}$\n",
    "- where the dimensions are nx164 and n rows represents the number of strains in the dataset and the 64 columns represent the intercept and the 63 columns for the dummy variables representing the effects and flavors of the strains.\n",
    "\n",
    "- $\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{63} \\end{bmatrix}$\n",
    "- where the dimensions are 64x1 and 64 rows represent the intercept nad coefficients for each of the 63 dummy variables and the 1 column represents the coefficient values.\n",
    "\n",
    "- $\\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}$\n",
    "- where the dimensions are nx1 and n rows represents the residuals for each strain and the 1 column represents the residual values.\n",
    "\n",
    "- Matrix Form:\n",
    "    - $Y = X\\beta + \\epsilon$\n",
    "    - $\\begin{bmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} = \\begin{bmatrix} 1 & X_{11} & X_{12} & \\dots & X_{163} \\\\ 1 & X_{21} & X_{22} & \\dots & X_{263} \\\\ \\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & X_{n1} & X_{n2} & \\dots & X_{n63} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{63} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Give the *equation* for the hat matrix for this model using a OLS. Where would you expect this equation to \"break\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $H = X(X^TX)^{-1}X^T$\n",
    "- The equation would break if the matrix $X^TX$ is not invertible. This would happen if any of the columns are highly correlated or linearly dependent. $X^TX$ would be singular and so the inverse would not exist.\n",
    "- The equation would also break if the number of observations is less than the number of coefficients. This would make the matrix $X^TX$ singular and the inverse would not exist. However from the csv file, I know that this is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Give the *equation* for the hat matrix for this model using a *Ridge penalty*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $H = X(X^TX + \\lambda I)^{-1}X^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Find the equation for the OLS hat matrix for this model predicting residuals. How does it compare to your hat matrix in A2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $H = X(X^TX)^{-1}X^T$\n",
    "- The hat matrix for predicting residuals is identical to the hat matrix in A2 and this is because the same design matrix and OLS formula is used. But this hat matrix is used to essentially map the observed residuals R to the predicted residuals $\\hat{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Would you recommend this strategy? Why or why not? Justify your answer mathematically.\n",
    "- I would not recommend this strategy because if you are predicting further from the design matrix, it would result in redundant predictions.\n",
    "- $R = Y - H_1Y$, where $H_1$ is the hat matrix for predicting Y and orthogonal to the design matrix.\n",
    "- So fitting a model to predict the residuals using the same design matrix is ineffective:\n",
    "- $H_2R = X(X^TX)^{-1}X^TR = 0$\n",
    "- This is because the residuals are orthogonal to the design matrix and so the predicted residuals would be 0.\n",
    "- So if $\\hat{R} = 0$, then the predicted house prices will be the same as the first model's predictions:\n",
    "- $\\hat{Y} = H_1Y - H_2R = H_1Y - 0 = H_1Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the data in, and drop all rows with missing data in the predictors. Decide if you want to standardize anything or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strain</th>\n",
       "      <th>Type</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Effects</th>\n",
       "      <th>Flavor</th>\n",
       "      <th>Creative</th>\n",
       "      <th>Energetic</th>\n",
       "      <th>Tingly</th>\n",
       "      <th>Euphoric</th>\n",
       "      <th>Relaxed</th>\n",
       "      <th>...</th>\n",
       "      <th>Ammonia</th>\n",
       "      <th>Minty</th>\n",
       "      <th>Tree</th>\n",
       "      <th>Fruit</th>\n",
       "      <th>Butter</th>\n",
       "      <th>Pineapple</th>\n",
       "      <th>Tar</th>\n",
       "      <th>Rose</th>\n",
       "      <th>Plum</th>\n",
       "      <th>Pear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100-Og</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n",
       "      <td>Earthy,Sweet,Citrus</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98-White-Widow</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n",
       "      <td>Flowery,Violet,Diesel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>sativa</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n",
       "      <td>Spicy/Herbal,Sage,Woody</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13-Dawgs</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n",
       "      <td>Apricot,Citrus,Grapefruit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24K-Gold</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n",
       "      <td>Citrus,Earthy,Orange</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Strain    Type  Rating                                     Effects  \\\n",
       "0          100-Og  hybrid     4.0  Creative,Energetic,Tingly,Euphoric,Relaxed   \n",
       "1  98-White-Widow  hybrid     4.7    Relaxed,Aroused,Creative,Happy,Energetic   \n",
       "2            1024  sativa     4.4   Uplifted,Happy,Relaxed,Energetic,Creative   \n",
       "3        13-Dawgs  hybrid     4.2     Tingly,Creative,Hungry,Relaxed,Uplifted   \n",
       "4        24K-Gold  hybrid     4.6   Happy,Relaxed,Euphoric,Uplifted,Talkative   \n",
       "\n",
       "                      Flavor  Creative  Energetic  Tingly  Euphoric  Relaxed  \\\n",
       "0        Earthy,Sweet,Citrus       1.0        1.0     1.0       1.0      1.0   \n",
       "1      Flowery,Violet,Diesel       1.0        1.0     0.0       0.0      1.0   \n",
       "2    Spicy/Herbal,Sage,Woody       1.0        1.0     0.0       0.0      1.0   \n",
       "3  Apricot,Citrus,Grapefruit       1.0        0.0     1.0       0.0      1.0   \n",
       "4       Citrus,Earthy,Orange       0.0        0.0     0.0       1.0      1.0   \n",
       "\n",
       "   ...  Ammonia  Minty  Tree  Fruit  Butter  Pineapple  Tar  Rose  Plum  Pear  \n",
       "0  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "1  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "2  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "3  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "4  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./cannabis_full.csv\")\n",
    "predictors = df.drop(columns=['Strain', 'Type', 'Effects', 'Flavor', 'Rating'])\n",
    "df_clean = df.dropna(subset=predictors.columns)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function to compute beta estimates for Ordinary Least Squares regression with multiple predictors. Run this function on the Cannabis data with three predictors: Type, Relaxed, and Energetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.74369181 0.0246247  0.06176504 0.61677751 0.3575477 ]\n"
     ]
    }
   ],
   "source": [
    "# Write a function to compute beta estimates for Ordinary Least Squares regression with multiple predictors. Run this function on the Cannabis data with three predictors: Type, Relaxed, and Energetic.\n",
    "def compute_betas(X, y):\n",
    "    betas = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return betas\n",
    "\n",
    "df_clean = pd.get_dummies(df_clean, columns=['Type'], drop_first=True)\n",
    "X = df_clean[['Type_indica', 'Type_sativa', 'Relaxed', 'Energetic']].values\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "y = df_clean['Rating'].values\n",
    "betas = compute_betas(X, y)\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function to compute beta estimates for Ridge regression, with $\\lambda$ as a user input. Use the following loss function:\n",
    "\n",
    "- $l(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_betas_ridge(X, y, lam):\n",
    "    betas = np.linalg.inv(X.T @ X + (X.shape[0] * lam) * np.eye(X.shape[1])) @ X.T @ y\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run your Ridge function on the Cannabis data with three predictors: Type, Relaxed, and Energetic with $\\lambda = 1000$. How do these estimates compare to your OLS estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0043112  0.00129269 0.00080465 0.00329303 0.00120866]\n"
     ]
    }
   ],
   "source": [
    "betas_ridge = compute_betas_ridge(X, y, 1000)\n",
    "print(betas_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The OLS estimates are higher than the Ridge estimates. This is because the Ridge estimates are penalized by the $\\lambda$ term and so the coefficients are shrunk towards 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run your Ridge function on the Cannabis data with all the predictors with $\\lambda = 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.29985189e-03 1.40274876e-03 1.20561358e-03 6.40154628e-04\n",
      " 3.08487327e-03 3.28431008e-03 3.77670869e-04 3.54424974e-03\n",
      " 2.84077793e-03 8.78311533e-04 6.78176397e-04 5.49476789e-04\n",
      " 1.12441374e-03 1.38053514e-03 1.73257123e-06 1.73257123e-06\n",
      " 2.10751664e-03 2.02122803e-03 1.00950921e-03 5.13166810e-04\n",
      " 1.37021178e-05 4.59673992e-04 4.33758626e-04 7.57394052e-05\n",
      " 4.79311197e-04 1.71994242e-05 7.25778317e-05 1.48435658e-04\n",
      " 8.61025001e-04 3.13815595e-04 6.48992150e-04 3.33098052e-04\n",
      " 6.85746076e-04 1.11409843e-04 4.51563533e-05 2.91136421e-04\n",
      " 1.19091611e-04 7.08955773e-05 6.13422437e-05 3.64210109e-04\n",
      " 1.11526604e-05 6.49251687e-05 4.78697692e-05 1.24918107e-05\n",
      " 3.18412027e-05 1.64985999e-05 2.95485558e-04 8.93771969e-05\n",
      " 2.78652659e-04 1.06391017e-04 2.85558389e-05 6.01444004e-05\n",
      " 7.14140539e-05 1.03959264e-04 4.62779443e-05 5.43182314e-05\n",
      " 8.15461080e-05 6.44130307e-05 6.44130307e-05 3.57667882e-05\n",
      " 8.11192791e-05 1.51631925e-05 3.04571745e-05 3.79965845e-06\n",
      " 5.74766318e-06 1.28929182e-03 8.02552982e-04]\n"
     ]
    }
   ],
   "source": [
    "predictors = df_clean.drop(columns=['Strain', 'Effects', 'Flavor', 'Rating'])\n",
    "X = predictors.values\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "betas_ridge = compute_betas_ridge(X, y, 1000)\n",
    "print(betas_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function to perform *tuning* on $\\lambda$, using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lambda_split(train, test, lam_values, metric):\n",
    "    X_train = train.drop(columns=['Rating']).values\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    y_train = train['Rating'].values\n",
    "    X_test = test.drop(columns=['Rating']).values\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    y_test = test['Rating'].values\n",
    "    metrics = []\n",
    "    for lam in lam_values:\n",
    "        betas = compute_betas_ridge(X_train, y_train, lam)\n",
    "        y_pred = X_test @ betas\n",
    "        if metric == 'r-sq':\n",
    "            y_bar = np.mean(y_test)\n",
    "            ss_tot = np.sum((y_test - y_bar) ** 2)\n",
    "            ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "            r2 = 1 - ss_res / ss_tot\n",
    "            metrics.append(r2)\n",
    "        elif metric == 'mse':\n",
    "            mse = np.mean((y_test - y_pred) ** 2)\n",
    "            metrics.append(mse)\n",
    "        elif metric == 'mae':\n",
    "            mae = np.mean(np.abs(y_test - y_pred))\n",
    "            metrics.append(mae)\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metrics})\n",
    "\n",
    "def tune_lambda(df, lam_values, metric, k):\n",
    "    n = df.shape[0]\n",
    "    fold_size = n // k\n",
    "    metrics = []\n",
    "    for lam in lam_values:\n",
    "        metric_values = []\n",
    "        for i in range(k):\n",
    "            test = df.iloc[i * fold_size:(i + 1) * fold_size]\n",
    "            train = df.drop(test.index)\n",
    "            df_metric = tune_lambda_split(train, test, [lam], metric)\n",
    "            metric_values.append(df_metric[metric].values[0])\n",
    "        metrics.append(np.mean(metric_values))\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use your tuning function to choose a best $\\lambda$ for Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "predictors = df_clean.drop(columns=['Strain', 'Effects', 'Flavor'])\n",
    "lam_values = np.logspace(0, 5, num=100)\n",
    "df_tune = tune_lambda(predictors, lam_values, 'r-sq', 5)\n",
    "best_lambda_rsq = df_tune.loc[df_tune['r-sq'].idxmax()]['lambda']\n",
    "print(best_lambda_rsq)\n",
    "\n",
    "df_tune = tune_lambda(predictors, lam_values, 'mse', 5)\n",
    "best_lambda_mse = df_tune.loc[df_tune['mse'].idxmin()]['lambda']\n",
    "print(best_lambda_mse)\n",
    "\n",
    "df_tune = tune_lambda(predictors, lam_values, 'mae', 5)\n",
    "best_lambda_mae = df_tune.loc[df_tune['mae'].idxmin()]['lambda']\n",
    "print(best_lambda_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seems that $\\lambda = 1$ is the best $\\lambda$ for Ridge regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
