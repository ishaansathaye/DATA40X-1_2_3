{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4 - Ishaan Sathaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following assume we have *p* predictors in our model, where *p* may be larger than 1, and *n* observations.\n",
    "\n",
    "Hint: You may want to use the notation: $sign(a) = {1 if a > 0, -1 if a < 0}$\n",
    "\n",
    "1. Give the *gradient* equation for Ordinary Least Squares Regression.\n",
    "    - loss function: $L(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2$\n",
    "    - $\\nabla L(\\beta) = \\frac{d}{d\\beta} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2$\n",
    "    - $\\nabla L(\\beta) = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i) x_i$\n",
    "    - In matrix form: $\\nabla L(\\beta) = \\frac{-2}{n} X^T (Y - \\hat{Y})$\n",
    "\n",
    "2. Give the *gradient* equation for Ridge Regression.\n",
    "    - loss function: $L(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n",
    "    - $\\nabla L(\\beta) = \\frac{d}{d\\beta} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n",
    "    - $\\nabla L(\\beta) = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i) x_i + 2 \\lambda \\beta$\n",
    "    - In matrix form: $\\nabla L(\\beta) = \\frac{-2}{n} X^T (Y - \\hat{Y}) + 2 \\lambda \\beta$\n",
    "\n",
    "3. Give the *gradient* equation for Lasso Regression.\n",
    "    - loss function: $L(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n",
    "    - $\\nabla L(\\beta) = \\frac{d}{d\\beta} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n",
    "    - $\\nabla L(\\beta) = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i) x_i + \\lambda sign(\\beta)$\n",
    "    - In matrix form: $\\nabla L(\\beta) = \\frac{-2}{n} X^T (Y - \\hat{Y}) + \\lambda sign(\\beta)$\n",
    "\n",
    "4. Give the *gradient* equation for Linear Regression with a loss function of $L(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^4 + \\lambda \\sum_{j=1}^{p} \\beta_j^4$\n",
    "    - $\\nabla L(\\beta) = -4 \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^3 x_i + 4 \\lambda \\beta^3$\n",
    "    - In matrix form: $\\nabla L(\\beta) = -4 X^T (Y - \\hat{Y})^3 + 4 \\lambda \\beta^3$\n",
    "\n",
    "5. Give the *gradient* equation for Linear Regression with a loss function of $L(\\beta) = \\sum_{i=1}^{n} |y_i - \\hat{y}_i| + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n",
    "    - $\\nabla L(\\beta) = - \\sum_{i=1}^{n} sign(y_i - \\hat{y}_i) x_i + \\lambda sign(\\beta)$\n",
    "    - In matrix form: $\\nabla L(\\beta) = - X^T sign(Y - \\hat{Y}) + \\lambda sign(\\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function to implement gradient descent for LASSO estimation on the cannabis data from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: fit_lasso\n",
    "# inputs: Y, X, lambda, eta\n",
    "\n",
    "#     initialize beta vector\n",
    "    \n",
    "#     while stopping condition is not met:\n",
    "    \n",
    "#         compute gradient values at current beta values\n",
    "        \n",
    "#         update beta = beta - eta*gradient\n",
    "        \n",
    "#         (optional) update eta\n",
    "        \n",
    "#         check for problematic iteration conditions\n",
    "#         # too many iterations, too much time, or clues that \n",
    "# the gradient descent is not converging well.\n",
    "        \n",
    "#     return beta estimates\n",
    "\n",
    "\n",
    "# Hint 1: Writing small helper functions, especially for \n",
    "# things like checking a stopping condition or checking the \n",
    "# iteration is converging, may help you cleanly keep track of \n",
    "# the steps in your code.\n",
    "\n",
    "# Hint 2: While you may not hard code information about the \n",
    "# cannabis dataset into your function, you may tailor your \n",
    "# function so that it works well on this dataset specifically. \n",
    "# For example, you can choose your eta - and whether you keep \n",
    "# the value constant or change it at each iteration - to be a \n",
    "# value that happens to converge well on this particular dataset.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fit_lasso(Y, X, lambda_, eta, stop_condition=1e-2, decay=0.9, max_iter=100):\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    eta_init = eta\n",
    "    for i in range(max_iter):\n",
    "        # compute gradient\n",
    "        gradient = compute_gradient(Y, X, lambda_, beta)\n",
    "        # exponential decay of learning rate\n",
    "        eta = eta_init * (decay ** i)\n",
    "        # update beta\n",
    "        beta = beta - eta*gradient\n",
    "        if check_stopping_condition(gradient, stop_condition):\n",
    "            break\n",
    "    return beta\n",
    "\n",
    "def compute_gradient(Y, X, lambda_, beta):\n",
    "    n = X.shape[0]\n",
    "    return -2/n * X.T @ (Y - X @ beta) + lambda_ * np.sign(beta)\n",
    "\n",
    "def check_stopping_condition(gradient, stop_condition):\n",
    "    return np.linalg.norm(gradient) < stop_condition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function to perform cross-validation on a set of lambdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lambda_split(train, test, lam_values, metric):\n",
    "    X_train = train.drop(columns=['Rating']).values\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    y_train = train['Rating'].values\n",
    "    X_test = test.drop(columns=['Rating']).values\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    y_test = test['Rating'].values\n",
    "    metrics = []\n",
    "    for lam in lam_values:\n",
    "        betas = fit_lasso(y_train, X_train, lam, 0.01, decay=0.75)\n",
    "        y_pred = X_test @ betas\n",
    "        if metric == 'r-sq':\n",
    "            y_bar = np.mean(y_test)\n",
    "            ss_tot = np.sum((y_test - y_bar) ** 2)\n",
    "            ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "            r2 = 1 - ss_res / ss_tot\n",
    "            metrics.append(r2)\n",
    "        elif metric == 'mse':\n",
    "            mse = np.mean((y_test - y_pred) ** 2)\n",
    "            metrics.append(mse)\n",
    "        elif metric == 'mae':\n",
    "            mae = np.mean(np.abs(y_test - y_pred))\n",
    "            metrics.append(mae)\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metrics})\n",
    "\n",
    "def tune_lambda(df, lam_values, metric, k):\n",
    "    n = df.shape[0]\n",
    "    fold_size = n // k\n",
    "    metrics = []\n",
    "    for lam in lam_values:\n",
    "        metric_values = []\n",
    "        for i in range(k):\n",
    "            test = df.iloc[i * fold_size:(i + 1) * fold_size]\n",
    "            train = df.drop(test.index)\n",
    "            df_metric = tune_lambda_split(train, test, [lam], metric)\n",
    "            metric_values.append(df_metric[metric].values[0])\n",
    "        metrics.append(np.mean(metric_values))\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply your cross-validation function to the cannabis dataset to find the “best” lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../hw3/cannabis_full.csv\")\n",
    "predictors = df.drop(columns=['Strain', 'Type', 'Effects', 'Flavor', 'Rating'])\n",
    "df_clean = df.dropna(subset=predictors.columns)\n",
    "df_clean = pd.get_dummies(df_clean, columns=['Type'], drop_first=True)\n",
    "# standardize the data\n",
    "predictors = df_clean.drop(columns=['Strain', 'Effects', 'Flavor', 'Rating'])\n",
    "predictors = (predictors - predictors.mean()) / predictors.std()\n",
    "predictors['Rating'] = df_clean['Rating']\n",
    "\n",
    "lam_values = np.logspace(0, 5, num=100)\n",
    "folds = 5\n",
    "df_tune_rsq = tune_lambda(predictors, lam_values, 'r-sq', folds)\n",
    "best_lambda_rsq = df_tune_rsq.loc[df_tune_rsq['r-sq'].idxmax()]['lambda']\n",
    "print(best_lambda_rsq)\n",
    "\n",
    "df_tune_mse = tune_lambda(predictors, lam_values, 'mse', folds)\n",
    "best_lambda_mse = df_tune_mse.loc[df_tune_mse['mse'].idxmin()]['lambda']\n",
    "print(best_lambda_mse)\n",
    "\n",
    "df_tune_mae = tune_lambda(predictors, lam_values, 'mae', folds)\n",
    "best_lambda_mae = df_tune_mae.loc[df_tune_mae['mae'].idxmin()]['lambda']\n",
    "print(best_lambda_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>r-sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-24.951810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.123324</td>\n",
       "      <td>-24.988616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.261857</td>\n",
       "      <td>-25.030070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.417474</td>\n",
       "      <td>-25.086174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.592283</td>\n",
       "      <td>-25.150598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>62802.914418</td>\n",
       "      <td>-28.851254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>70548.023107</td>\n",
       "      <td>-28.851254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>79248.289835</td>\n",
       "      <td>-28.851254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>89021.508545</td>\n",
       "      <td>-28.851254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>-28.851254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lambda       r-sq\n",
       "0        1.000000 -24.951810\n",
       "1        1.123324 -24.988616\n",
       "2        1.261857 -25.030070\n",
       "3        1.417474 -25.086174\n",
       "4        1.592283 -25.150598\n",
       "..            ...        ...\n",
       "95   62802.914418 -28.851254\n",
       "96   70548.023107 -28.851254\n",
       "97   79248.289835 -28.851254\n",
       "98   89021.508545 -28.851254\n",
       "99  100000.000000 -28.851254\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tune_rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000.0\n",
      "-9.80247941635426e-05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = predictors.drop(columns=['Rating']).values\n",
    "y = predictors['Rating'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "lasso = LassoCV(alphas=lam_values, cv=5)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "best_lambda_sklearn = lasso.alpha_\n",
    "print(best_lambda_sklearn)\n",
    "\n",
    "# eval on test set\n",
    "test_score = lasso.score(X_test, y_test)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
