{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4 - Ishaan Sathaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following assume we have *p* predictors in our model, where *p* may be larger than 1, and *n* observations.\n",
    "\n",
    "Hint: You may want to use the notation: $sign(a) = {1 if a > 0, -1 if a < 0}$\n",
    "\n",
    "1. Give the *gradient* equation for Ordinary Least Squares Regression.\n",
    "    - loss function: $L(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2$\n",
    "    - $\\nabla L(\\beta) = \\frac{d}{d\\beta} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2$\n",
    "    - $\\nabla L(\\beta) = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i) x_i$\n",
    "    - In matrix form: $\\nabla L(\\beta) = \\frac{-2}{n} X^T (Y - \\hat{Y})$\n",
    "\n",
    "2. Give the *gradient* equation for Ridge Regression.\n",
    "    - loss function: $L(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n",
    "    - $\\nabla L(\\beta) = \\frac{d}{d\\beta} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n",
    "    - $\\nabla L(\\beta) = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i) x_i + 2 \\lambda \\beta$\n",
    "    - In matrix form: $\\nabla L(\\beta) = \\frac{-2}{n} X^T (Y - \\hat{Y}) + 2 \\lambda \\beta$\n",
    "\n",
    "3. Give the *gradient* equation for Lasso Regression.\n",
    "    - loss function: $L(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n",
    "    - $\\nabla L(\\beta) = \\frac{d}{d\\beta} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n",
    "    - $\\nabla L(\\beta) = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta^T x_i) x_i + \\lambda sign(\\beta)$\n",
    "    - In matrix form: $\\nabla L(\\beta) = \\frac{-2}{n} X^T (Y - \\hat{Y}) + \\lambda sign(\\beta)$\n",
    "\n",
    "4. Give the *gradient* equation for Linear Regression with a loss function of $L(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^4 + \\lambda \\sum_{j=1}^{p} \\beta_j^4$\n",
    "    - $\\nabla L(\\beta) = -4 \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^3 x_i + 4 \\lambda \\beta^3$\n",
    "    - In matrix form: $\\nabla L(\\beta) = -4 X^T (Y - \\hat{Y})^3 + 4 \\lambda \\beta^3$\n",
    "\n",
    "5. Give the *gradient* equation for Linear Regression with a loss function of $L(\\beta) = \\sum_{i=1}^{n} |y_i - \\hat{y}_i| + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n",
    "    - $\\nabla L(\\beta) = - \\sum_{i=1}^{n} sign(y_i - \\hat{y}_i) x_i + \\lambda sign(\\beta)$\n",
    "    - In matrix form: $\\nabla L(\\beta) = - X^T sign(Y - \\hat{Y}) + \\lambda sign(\\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function to implement gradient descent for LASSO estimation on the cannabis data from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fit_lasso(Y, X, lambda_, eta, stop_condition):\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    current_beta = beta\n",
    "    while True:\n",
    "        # compute gradient\n",
    "        gradient = compute_gradient(Y, X, lambda_, beta)\n",
    "        # update beta\n",
    "        beta = beta - eta*gradient\n",
    "        # check stopping condition\n",
    "        if check_stopping_condition_beta(beta, current_beta, stop_condition):\n",
    "            break\n",
    "        current_beta = beta\n",
    "    return beta\n",
    "\n",
    "def compute_gradient(Y, X, lambda_, beta):\n",
    "    n = X.shape[0]\n",
    "    return -2/n * X.T @ (Y - (X @ beta)) + lambda_ * np.sign(beta)\n",
    "\n",
    "def check_stopping_condition_beta(beta, previous_beta, stop_condition):\n",
    "    return np.linalg.norm(beta - previous_beta) < stop_condition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function to perform cross-validation on a set of lambdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lambda_split(train, test, lam, metric):\n",
    "    X_train = train.drop(columns=['Rating']).values\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    y_train = train['Rating'].values\n",
    "    X_test = test.drop(columns=['Rating']).values\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    y_test = test['Rating'].values\n",
    "    betas = fit_lasso(y_train, X_train, lam, eta=0.1, stop_condition=1e-3)\n",
    "    y_pred = X_test @ betas  # Test set predictions\n",
    "    \n",
    "    if metric == 'r-sq':\n",
    "        y_bar = np.mean(y_test)\n",
    "        ss_tot = np.sum((y_test - y_bar) ** 2)\n",
    "        ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "        r2 = 1 - ss_res / ss_tot\n",
    "        return r2\n",
    "    elif metric == 'mse':\n",
    "        mse = np.mean((y_test - y_pred) ** 2)\n",
    "        return mse\n",
    "    elif metric == 'mae':\n",
    "        mae = np.mean(np.abs(y_test - y_pred))\n",
    "        return mae\n",
    "\n",
    "def tune_lambda(df, lam_values, metric, k):\n",
    "    n = df.shape[0]\n",
    "    fold_size = n // k\n",
    "    remainder = n % k\n",
    "    metrics = []\n",
    "    for lam in lam_values:\n",
    "        metric_values = []\n",
    "        for i in range(k):\n",
    "            start_idx = i * fold_size\n",
    "            if i == k - 1:\n",
    "                end_idx = (i + 1) * fold_size + remainder\n",
    "            else:\n",
    "                end_idx = (i + 1) * fold_size\n",
    "            test = df.iloc[start_idx:end_idx]\n",
    "            train = df.drop(test.index)\n",
    "            fold_metric = tune_lambda_split(train, test, lam, metric)\n",
    "            metric_values.append(fold_metric)\n",
    "        metrics.append(np.mean(metric_values))\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply your cross-validation function to the cannabis dataset to find the “best” lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../hw3/cannabis_full.csv\")\n",
    "predictors = df.drop(columns=['Strain', 'Type', 'Effects', 'Flavor', 'Rating'])\n",
    "df_clean = df.dropna(subset=predictors.columns)\n",
    "df_clean = pd.get_dummies(df_clean, columns=['Type'], drop_first=True)\n",
    "# standardize the data\n",
    "predictors = df_clean.drop(columns=['Strain', 'Effects', 'Flavor', 'Rating'])\n",
    "predictors = (predictors - predictors.mean()) / predictors.std()\n",
    "predictors['Rating'] = df_clean['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.001\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "lam_values = [0.001, 0.0001, 0.00001, 0.000001]\n",
    "folds = 5\n",
    "df_tune_rsq = tune_lambda(predictors, lam_values, 'r-sq', folds)\n",
    "best_lambda_rsq = df_tune_rsq.loc[df_tune_rsq['r-sq'].idxmax()]['lambda']\n",
    "print(best_lambda_rsq)\n",
    "\n",
    "df_tune_mse = tune_lambda(predictors, lam_values, 'mse', folds)\n",
    "best_lambda_mse = df_tune_mse.loc[df_tune_mse['mse'].idxmin()]['lambda']\n",
    "print(best_lambda_mse)\n",
    "\n",
    "df_tune_mae = tune_lambda(predictors, lam_values, 'mae', folds)\n",
    "best_lambda_mae = df_tune_mae.loc[df_tune_mae['mae'].idxmin()]['lambda']\n",
    "print(best_lambda_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the best lambda is 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fit your final LASSO model, using the \"best\" lambda, on the cannabis dataset. Interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest 3 coefficients:\n",
      "Intercept: 4.3183280649557405\n",
      "Relaxed: 0.23746128314044956\n",
      "Creative: 0.20452049803384287\n"
     ]
    }
   ],
   "source": [
    "# 4. Fit your final LASSO model, using the \"best\" lambda, on the cannabis dataset. Interpret the results.\n",
    "\n",
    "X = predictors.drop(columns=['Rating']).values\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "y = predictors['Rating'].values\n",
    "betas = fit_lasso(y, X, best_lambda_rsq, eta=0.1, stop_condition=1e-3)\n",
    "\n",
    "predictor_names = predictors.columns\n",
    "coefficients_with_names = list(zip(betas, ['Intercept'] + list(predictor_names)))\n",
    "largest_coefficients = sorted(coefficients_with_names, key=lambda x: abs(x[0]), reverse=True)[:3]\n",
    "print(\"Largest 3 coefficients:\")\n",
    "for coef, name in largest_coefficients:\n",
    "    print(f\"{name}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 3 largest coefficients and the interpretation of the results of the model being fit on the dataset.\n",
    "\n",
    "Since the predictors are standardized,  the coefficients can be interpreted as the expected change in the response variable for a one standard deviation change in the predictor.\n",
    "\n",
    "- The intercept is 4.32, which is the expected value of the response variable when all predictors are 0.\n",
    "- For a one standard deviation increase in the Relaxed predictor, the expected value of the response variable increases by 0.24.\n",
    "- For a one standard deviation increase in the Creative predictor, the expected value of the response variable increases by 0.20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In class, when performing gradient descent to find LASSO estimates, we used as our initial values: $\\beta = (0, 0, ..., 0)^T$. Suggest three different ways to choose the initial $\\beta$'s. Give some intuition for hjwy each one might be better than using all 0's.\n",
    "- The first way is to initialize the $\\beta$'s with really small random values. This could lead to a much faster convergence since there are different start points for the algorithm to converge to the minimum.\n",
    "- The second way way is to initialize the $\\beta$'s with the OLS estimates. Once the initial estimates are found, the gradient descent can start here are further optimize the values.\n",
    "- The third way is make educated guesses and this can be done through some initial information or intuition about the problem and its individual predictors. If we have this prior knowledge then better initial estimates can be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider the model:\n",
    "\n",
    "house price = $\\beta_0 + \\beta_1 *$ size in square feet $ +   \\epsilon$\n",
    "\n",
    "with a squared-error loss and Ridge penalty loss function.\n",
    "\n",
    "For the small example data:\n",
    "\n",
    "sq footage | 1500 | 2200 | 3700 | 4100\n",
    "\n",
    "price (in millions) | 1 | 2 | 3 | 14\n",
    "\n",
    "Show the first 2 gradient descent updates with:\n",
    "- $\\eta = 0.01$ \n",
    "- $\\eta = 1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta = 0.01\n",
      "gradient after iteration 0: [-1.00e+01 -3.72e+04]\n",
      "beta after iteration 0: [1.00e-01 3.72e+02]\n",
      "gradient after iteration 1: [2.13899040e+06 6.99170412e+09]\n",
      "beta after iteration 1: [-2.13898040e+04 -6.99166692e+07]\n",
      "[-2.13898040e+04 -6.99166692e+07]\n",
      "\n",
      "eta = 1.0\n",
      "gradient after iteration 0: [-1.00e+01 -3.72e+04]\n",
      "beta after iteration 0: [1.00e+01 3.72e+04]\n",
      "gradient after iteration 1: [2.13900030e+08 6.99174095e+11]\n",
      "beta after iteration 1: [-2.13900020e+08 -6.99174058e+11]\n",
      "[-2.13900020e+08 -6.99174058e+11]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 1500], [1, 2200], [1, 3700], [1, 4100]])\n",
    "y = np.array([1, 2, 3, 14])\n",
    "\n",
    "def fit_lasso_c(X, y, eta, lam, max_iter=2, stop_condition=1e-3):\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    current_beta = beta\n",
    "    for i in range(max_iter):\n",
    "        # compute gradient\n",
    "        gradient = compute_gradient_c(y, X, beta, lam)\n",
    "        print(f\"gradient after iteration {i}: {gradient}\")\n",
    "        # update beta\n",
    "        beta = beta - eta*gradient\n",
    "        print(f\"beta after iteration {i}: {beta}\")\n",
    "        # check stopping condition\n",
    "        if check_stopping_condition_beta(beta, current_beta, stop_condition):\n",
    "            break\n",
    "        current_beta = beta\n",
    "    return beta\n",
    "\n",
    "def compute_gradient_c(Y, X, beta, lambda_):\n",
    "    n = X.shape[0]\n",
    "    # using squared error loss and ridge penalty term:\n",
    "    return -2/n * X.T @ (Y - (X @ beta)) + 2 * lambda_ * beta\n",
    "\n",
    "print(\"eta = 0.01\")\n",
    "a = fit_lasso_c(X, y, 0.01, 1)\n",
    "print(a)\n",
    "print()\n",
    "print(\"eta = 1.0\")\n",
    "b = fit_lasso_c(X, y, 1.0, 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 2 gradient descent updates with $\\eta = 0.01$ are:\n",
    "- gradient after iteration 0: [-1.00e+01 -3.72e+04]\n",
    "- new guesses after iteration 0: [1.00e-01 3.72e+02]\n",
    "- gradient after iteration 1: [2.13899040e+06 6.99170412e+09]\n",
    "- new guesses after iteration 1: [-2.13898040e+04 -6.99166692e+07]\n",
    "\n",
    "The first 2 gradient descent updates with $\\eta = 1.0$ are:\n",
    "- gradient after iteration 0: [-1.00e+01 -3.72e+04]\n",
    "- new guesses after iteration 0: [1.00e+01 3.72e+04]\n",
    "- gradient after iteration 1: [2.13900030e+08 6.99174095e+11]\n",
    "- new guesses after iteration 1: [-2.13900020e+08 -6.99174058e+11]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
