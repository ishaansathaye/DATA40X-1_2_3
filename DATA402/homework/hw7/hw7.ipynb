{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7 - Ishaan Sathaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the following, you might find the following notation useful:*\n",
    "\n",
    "$1_{A, i1} = 1 \\text{ if observation i is in class A}$\n",
    "\n",
    "$1_{A, i1} = 0 \\text{ if observation i is not in class A}$\n",
    "\n",
    "Consider this Neural Network, in which we have:\n",
    "- Three sets of weights and biases for the initial layer, $\\overrightarrow{w_1}$, $\\overrightarrow{w_2}$, $\\overrightarrow{w_3}$; $c_1$, $c_2$, $c_3$\n",
    "- One weight set and bias for the hidden layer, $\\overrightarrow{v} = (v_1, v_2, v_3)$; $d$\n",
    "- A sigmoid activation function, $g(u) = \\frac{1}{1 + e^{-u}}$\n",
    "- A loss function of squared-error on the predicted probabilities of Class 1, compared to y values of 0 or 1: \n",
    "\n",
    "$$L = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{p_i} - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 Second Layer Gradients\n",
    "\n",
    "Give the gradient equation for $v_1$, $v_2$, $v_3$, and $d$. (When possible feel free to say \"the rest is the same as for the other one\" rather than repeating equations)\n",
    "\n",
    "- Gradient for $v_j$ is the same as for $v_1$ for $j = 2, 3$\n",
    "    - $u = v_1g_1 + v_2g_2 + v_3g_3 + d$\n",
    "    - $g_j = g(w^T_jx_i + c_j)$\n",
    "    - $\\frac{\\partial L}{\\partial v_j} = \\frac{2}{n}\\sum_{i=1}^{n} (\\hat{p_i} - y_i)(\\hat{p_i})(1 - \\hat{p_i})g_j$\n",
    "- Gradient for $d$:\n",
    "    - $\\frac{\\partial L}{\\partial d} = \\frac{2}{n}\\sum_{i=1}^{n} (\\hat{p_i} - y_i)(\\hat{p_i})(1 - \\hat{p_i})$\n",
    "\n",
    "### Question 2 First Layer Gradients\n",
    "\n",
    "Give the gradient equations for $w_{1, 1}$, $w_{1, 2}$, ..., $w_{3, p}$, $c_1$, $c_2$, $c_3$.\n",
    "\n",
    "- Gradient for $w_{j, k}$:\n",
    "    - $\\frac{\\partial L}{\\partial w_{j, k}} = \\frac{2}{n}\\sum_{i=1}^{n} (\\hat{p_i} - y_i)(\\hat{p_i})(1 - \\hat{p_i})v_jg^{\\prime}_j(w^T_jx_i + c_j)x_{i, k}$\n",
    "        - the term $g^{\\prime}_j(w^T_jx_i + c_j)$ is equal to $ g(w^T_jx_i + c_j)(1 - g(w^T_jx_i + c_j))$\n",
    "- Gradient for $c_j$:\n",
    "    - $\\frac{\\partial L}{\\partial c_j} = \\frac{2}{n}\\sum_{i=1}^{n} (\\hat{p_i} - y_i)(\\hat{p_i})(1 - \\hat{p_i})v_jg^{\\prime}_j(w^T_jx_i + c_j)$\n",
    "    - the rest is the same as for the other ones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Computer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strain</th>\n",
       "      <th>Type</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Creative</th>\n",
       "      <th>Energetic</th>\n",
       "      <th>Tingly</th>\n",
       "      <th>Euphoric</th>\n",
       "      <th>Relaxed</th>\n",
       "      <th>Aroused</th>\n",
       "      <th>Happy</th>\n",
       "      <th>...</th>\n",
       "      <th>Ammonia</th>\n",
       "      <th>Minty</th>\n",
       "      <th>Tree</th>\n",
       "      <th>Fruit</th>\n",
       "      <th>Butter</th>\n",
       "      <th>Pineapple</th>\n",
       "      <th>Tar</th>\n",
       "      <th>Rose</th>\n",
       "      <th>Plum</th>\n",
       "      <th>Pear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3-Bears-Og</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>303-Og</td>\n",
       "      <td>0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3D-Cbd</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3X-Crazy</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Strain  Type  Rating  Creative  Energetic  Tingly  Euphoric  Relaxed  \\\n",
       "2        1024     1     4.4       1.0        1.0     0.0       0.0      1.0   \n",
       "5  3-Bears-Og     0     0.0       0.0        0.0     0.0       0.0      0.0   \n",
       "7      303-Og     0     4.2       0.0        0.0     0.0       1.0      1.0   \n",
       "8      3D-Cbd     1     4.6       0.0        0.0     0.0       0.0      1.0   \n",
       "9    3X-Crazy     0     4.4       0.0        0.0     1.0       1.0      1.0   \n",
       "\n",
       "   Aroused  Happy  ...  Ammonia  Minty  Tree  Fruit  Butter  Pineapple  Tar  \\\n",
       "2      0.0    1.0  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   \n",
       "5      0.0    0.0  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   \n",
       "7      0.0    1.0  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   \n",
       "8      0.0    1.0  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   \n",
       "9      0.0    1.0  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   \n",
       "\n",
       "   Rose  Plum  Pear  \n",
       "2   0.0   0.0   0.0  \n",
       "5   0.0   0.0   0.0  \n",
       "7   0.0   0.0   0.0  \n",
       "8   0.0   0.0   0.0  \n",
       "9   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cannabis = pd.read_csv('../hw3/cannabis_full.csv')\n",
    "cannabis_hybrid = cannabis[cannabis['Type'] == 'hybrid']\n",
    "\n",
    "# remove hybrid strains\n",
    "cannabis = cannabis[cannabis['Type'] != 'hybrid']\n",
    "\n",
    "# remove effects and flavors\n",
    "cannabis = cannabis.drop(columns=['Effects', 'Flavor'])\n",
    "\n",
    "# map type to 0 or 1\n",
    "cannabis['Type'] = cannabis['Type'].map({'indica': 0, 'sativa': 1})\n",
    "\n",
    "# drop null values\n",
    "cannabis = cannabis.dropna()\n",
    "\n",
    "cannabis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "effect_cols = cannabis.columns[cannabis.columns.get_loc(\"Creative\"):cannabis.columns.get_loc(\"Mouth\")+1]\n",
    "flavors_cols = cannabis.columns[cannabis.columns.get_loc(\"Earthy\"):cannabis.columns.get_loc(\"Pear\")+1]\n",
    "\n",
    "# Logistic Regression using only effect predictors\n",
    "m1 = LogisticRegression()\n",
    "m1.fit(cannabis[effect_cols], cannabis['Type'])\n",
    "\n",
    "# Logistic Regression using only flavor predictors\n",
    "m2 = LogisticRegression()\n",
    "m2.fit(cannabis[flavors_cols], cannabis['Type'])\n",
    "\n",
    "# Logistic Regression using only rating predictor\n",
    "m3 = LogisticRegression()\n",
    "m3.fit(cannabis[['Rating']], cannabis['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w1 = np.concatenate([m1.coef_[0], np.zeros(len(flavors_cols) + 1)])\n",
    "w2 = np.concatenate([np.zeros(len(effect_cols)), m2.coef_[0], np.zeros(1)])\n",
    "w3 = np.concatenate([np.zeros(len(effect_cols) + len(flavors_cols)), m3.coef_[0]])\n",
    "\n",
    "c1 = m1.intercept_[0]\n",
    "c2 = m2.intercept_[0]\n",
    "c3 = m3.intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Optimizing the final layer\n",
    "\n",
    "Update your code from Homework 6 so that the final layer parameters \n",
    "are chosen via gradient descent. (Keep the “cheater” w’s you that you implemented last week.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    sig = sigmoid(z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "# stopping condition\n",
    "def check_stopping_condition(v, d, threshold=1e-4):\n",
    "    return np.linalg.norm(v) < threshold and np.abs(d) < threshold\n",
    "\n",
    "def loss(output, target):\n",
    "    return (output - target) ** 2\n",
    "\n",
    "def gd_perceptron(train_pred, train_target, w1, w2, w3, c1, c2, c3, learning_rate=0.01, epochs=1000):\n",
    "    # initialize v and d\n",
    "    v = np.array([1/3, 1/3, 1/3])\n",
    "    d = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_loss = 0\n",
    "        for preds, target in zip(train_pred, train_target):\n",
    "            preds = np.array(preds)\n",
    "            z1 = np.dot(preds, w1) + c1\n",
    "            z2 = np.dot(preds, w2) + c2\n",
    "            z3 = np.dot(preds, w3) + c3\n",
    "            u1 = sigmoid(z1)\n",
    "            u2 = sigmoid(z2)\n",
    "            u3 = sigmoid(z3)\n",
    "\n",
    "            u = np.array([u1, u2, u3])\n",
    "            \n",
    "            # hidden layer\n",
    "            hidden_layer_input = np.dot(u, v) + d\n",
    "            output = sigmoid(hidden_layer_input)\n",
    "            \n",
    "            # loss\n",
    "            # pred_type = 1 if output >= 0.5 else 0\n",
    "            current_loss = loss(output, target)\n",
    "            total_loss += current_loss\n",
    "            \n",
    "            # backpropagation\n",
    "            output_error = output - target\n",
    "            deriv = sigmoid_derivative(output)\n",
    "            \n",
    "            grad_v = output_error * deriv * u\n",
    "            grad_d = output_error * deriv\n",
    "            \n",
    "            if check_stopping_condition(grad_v, grad_d, 1e-4):\n",
    "                break\n",
    "            \n",
    "            v -= learning_rate * grad_v\n",
    "            d -= learning_rate * grad_d\n",
    "\n",
    "    return v, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Best final layer\n",
    "\n",
    "Fit your function on the same cannabis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6.17362546,  4.19162658, -1.16314   ]), -4.340883764684458)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cannabis_preds = cannabis[effect_cols.tolist() + flavors_cols.tolist() + ['Rating']].values\n",
    "true_labels = cannabis['Type'].values\n",
    "\n",
    "v, d = gd_perceptron(cannabis_preds, true_labels, w1, w2, w3, c1, c2, c3)\n",
    "v,d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Optimizing everything\n",
    "\n",
    "Now update your code so that all parameters are chosen via gradient descent and back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stopping_condition_all(v, d, w1, w2, w3, c1, c2, c3, threshold=1e-4):\n",
    "    return (np.linalg.norm(v) < threshold and np.abs(d) < threshold and\n",
    "    np.linalg.norm(w1) < threshold and np.linalg.norm(w2) < threshold and\n",
    "    np.linalg.norm(w3) < threshold and np.abs(c1) < threshold and\n",
    "    np.abs(c2) < threshold and np.abs(c3) < threshold)\n",
    "\n",
    "def gd_perceptron_all(train_pred, train_target, w1, w2, w3, c1, c2, c3, learning_rate=0.01, epochs=500):\n",
    "    # initialize v and d\n",
    "    v = np.array([1/3, 1/3, 1/3])\n",
    "    d = 0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for preds, target in zip(train_pred, train_target):\n",
    "            preds = np.array(preds)\n",
    "            \n",
    "            # forward pass\n",
    "            z1 = np.dot(preds, w1) + c1\n",
    "            z2 = np.dot(preds, w2) + c2\n",
    "            z3 = np.dot(preds, w3) + c3\n",
    "            u1 = sigmoid(z1)\n",
    "            u2 = sigmoid(z2)\n",
    "            u3 = sigmoid(z3)\n",
    "            u = np.array([u1, u2, u3])\n",
    "            \n",
    "            # hidden layer\n",
    "            hidden_layer_input = np.dot(u, v) + d\n",
    "\n",
    "            output = sigmoid(hidden_layer_input)\n",
    "            \n",
    "            # loss\n",
    "            current_loss = loss(output, target)\n",
    "            total_loss += current_loss\n",
    "            \n",
    "            # backpropagation\n",
    "            output_error = output - target\n",
    "            deriv = sigmoid_derivative(output)\n",
    "            \n",
    "            # gradients for v and d\n",
    "            grad_v = output_error * deriv * u\n",
    "            grad_d = output_error * deriv\n",
    "            \n",
    "            grad_u = output_error * deriv * v\n",
    "            grad_z1 = grad_u[0] * sigmoid_derivative(z1)\n",
    "            grad_z2 = grad_u[1] * sigmoid_derivative(z2)\n",
    "            grad_z3 = grad_u[2] * sigmoid_derivative(z3)\n",
    "            grad_w1 = grad_z1 * preds\n",
    "            grad_w2 = grad_z2 * preds\n",
    "            grad_w3 = grad_z3 * preds\n",
    "            grad_c1 = grad_z1\n",
    "            grad_c2 = grad_z2\n",
    "            grad_c3 = grad_z3\n",
    "            \n",
    "            w1 -= learning_rate * grad_w1\n",
    "            w2 -= learning_rate * grad_w2\n",
    "            w3 -= learning_rate * grad_w3\n",
    "            c1 -= learning_rate * grad_c1\n",
    "            c2 -= learning_rate * grad_c2\n",
    "            c3 -= learning_rate * grad_c3\n",
    "            v -= learning_rate * grad_v\n",
    "            d -= learning_rate * grad_d\n",
    "\n",
    "        if check_stopping_condition_all(v, d, w1, w2, w3, c1, c2, c3, 1e-4):\n",
    "            break\n",
    "\n",
    "    return w1, w2, w3, c1, c2, c3, v, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 Fit\n",
    "\n",
    "Fit your Neural Network on the cannabis data.\n",
    "\n",
    "(You will not lose points on this problem if your function does not successfully converge; however, I need to see that it is successfully running at least a few backprop steps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1: [-0.71683973  2.98361483 -1.13090688 -0.25081201 -1.3406478   0.17277603\n",
      "  0.94300489 -0.3105249   0.57014961  1.30562695 -1.18385025  0.67749114\n",
      " -2.63490791  0.          0.          0.16610538  0.54686096  1.43002331\n",
      "  0.06842544 -0.06504742  0.59203785 -0.85776847  0.47336835 -0.91235034\n",
      "  0.11868051 -0.37160788 -0.18106447  0.28056321 -1.23754765  0.57814552\n",
      " -0.09365098  0.00646762  0.30968857  0.31165338 -0.82152614  1.08374577\n",
      "  0.21346203  0.06897588  0.07067305  0.46164007  0.16819764 -0.77970105\n",
      " -0.4219564  -0.07757994  0.20624473  0.08880509 -0.338095   -1.45516466\n",
      " -0.59016466  0.0114926   0.04636659  0.11367954  0.07131531  0.78121495\n",
      " -0.26777455 -0.34529515 -0.29139766 -0.29139766 -0.47090873  1.04674478\n",
      "  0.8892645  -0.0827357   0.50538801 -0.02442583  0.11189354]\n",
      "w2: [ 1.06450855 -0.03103494  0.81646611 -0.2049515  -0.9872233  -0.67497826\n",
      " -0.25353403  1.38377553 -1.02675742  0.07324382  0.00399838  1.31672537\n",
      " -0.40019272  0.          0.         -0.00383132  0.11247696 -0.11790633\n",
      " -0.95310943  0.42097865  0.92664024  0.91383766  0.1848128   0.38926831\n",
      "  1.63199994  0.72010163  1.15621815 -1.18879971 -1.14508147 -0.30378735\n",
      "  0.19416938 -0.92457994 -0.79891725 -0.18395466  0.74682932 -0.60275298\n",
      "  0.33755323  0.70375702  0.78142495  0.39727889  0.21367641 -0.90707047\n",
      " -1.26535766  0.70725135 -0.14326176  1.0325022   1.06890509 -0.00977085\n",
      " -0.47314717  0.1269541  -1.57592055 -0.66896753  0.19712719 -0.42090498\n",
      " -0.9255905  -0.0467475   0.42286666  0.42286666 -1.84229668  1.96668933\n",
      "  1.18781973 -0.84969528  0.14509415 -0.39076832  0.09553247]\n",
      "w3: [-0.00316988 -0.35704534  0.20277953 -0.02026589  0.1086787  -0.27299178\n",
      " -0.04389238  0.09126669 -0.01078604 -0.19103487  0.16208621  0.24604802\n",
      " -0.07307502  0.          0.         -0.12718023  0.24403808  0.28584546\n",
      " -0.18264626  0.00165479 -0.06684495  0.00174129  0.04332867 -0.23543346\n",
      " -0.21512655 -0.03737333 -0.21452675 -0.00089282  0.02823531  0.03758622\n",
      " -0.1417162  -0.06118829  0.14893324  0.15240117 -0.41867969  0.02062737\n",
      " -0.20171128 -0.05199453 -0.02878504 -0.05690205 -0.00822025  0.16697299\n",
      "  0.05883053 -0.02389096  0.00136731 -0.16762332 -0.028629   -0.32861881\n",
      "  0.03634985 -0.07389198  0.21275861  0.04526623 -0.02054795  0.06962269\n",
      "  0.10974472 -0.00829821  0.02919795  0.02919795  0.13425862 -0.1541579\n",
      " -0.14939502  0.07194375 -0.05521903  0.00808614  0.25223741]\n",
      "c1: 0.36137368435839057\n",
      "c2: 0.1470695705640188\n",
      "c3: -0.289264094943774\n",
      "v: [ 5.42277772  4.3876175  -1.70228648]\n",
      "d: -4.842660198868449\n"
     ]
    }
   ],
   "source": [
    "w1, w2, w3, c1, c2, c3, v, d = gd_perceptron_all(cannabis_preds, true_labels, w1, w2, w3, c1, c2, c3)\n",
    "\n",
    "print(\"w1:\", w1)\n",
    "print(\"w2:\", w2)\n",
    "print(\"w3:\", w3)\n",
    "print(\"c1:\", c1)\n",
    "print(\"c2:\", c2)\n",
    "print(\"c3:\", c3)\n",
    "print(\"v:\", v)\n",
    "print(\"d:\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "- In B2, what values of weights in the final layer were chosen? What does this tell you about the three regressions you fit to make the \"cheater\" $w$'s?\n",
    "    - The weights for the final layer were 6.17, 4.19, -1.1, and d = -4.34. The first hidden node has the most weight, essentially meaning it has the most influence on the output, where the second and third have moderate to little influence.\n",
    "    - This tells us that the first regression was the most important, indicating that the effects features were the most predictive of the cannabis type. And then the flavor features were much more predictive than the rating features. This is also pretty consistent with our understanding of the data, as the effects are the most indicative of the type of cannabis, followed by the flavor, and then the rating.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "- Use your fitted model from B4 to predict on the data, and make a confusion matrix. How did this model perform compare to our \"cheater\" one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[619  68]\n",
      " [ 50 381]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict using the fitted parameters\n",
    "def predict_with_fitted_model(preds, w1, w2, w3, c1, c2, c3, v, d):\n",
    "    preds = np.array(preds)\n",
    "    z1 = np.dot(preds, w1) + c1\n",
    "    z2 = np.dot(preds, w2) + c2\n",
    "    z3 = np.dot(preds, w3) + c3\n",
    "    u1 = sigmoid(z1)\n",
    "    u2 = sigmoid(z2)\n",
    "    u3 = sigmoid(z3)\n",
    "    u = np.array([u1, u2, u3])\n",
    "    # hidden layer\n",
    "    hidden_layer_input = np.dot(u, v) + d\n",
    "    output = sigmoid(hidden_layer_input)\n",
    "    return output * 100\n",
    "\n",
    "# Predict on the data\n",
    "predictions = [predict_with_fitted_model(p, w1, w2, w3, c1, c2, c3, v, d) for p in cannabis_preds]\n",
    "\n",
    "predictions = [1 if p > 50 else 0 for p in predictions]\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cheater Confusion Matrix:\n",
      "[[656  31]\n",
      " [147 284]]\n"
     ]
    }
   ],
   "source": [
    "# cheater model confusion matrix\n",
    "train_predicted = cannabis[effect_cols.tolist() + flavors_cols.tolist() + ['Rating']]\n",
    "preds = []\n",
    "for i in range(len(train_predicted)):\n",
    "    effects_preds = train_predicted.iloc[i][effect_cols]\n",
    "    flavors_preds = train_predicted.iloc[i][flavors_cols]\n",
    "    rating_pred = train_predicted.iloc[i]['Rating']\n",
    "\n",
    "    # supress warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    p_effects = m1.predict_proba([effects_preds])[0][1]\n",
    "    p_flavors = m2.predict_proba([flavors_preds])[0][1]\n",
    "    p_rating = m3.predict_proba([[rating_pred]])[0][1]\n",
    "\n",
    "    p = (p_effects + p_flavors + p_rating) / 3\n",
    "    preds.append(1 if p > 0.5 else 0)\n",
    "\n",
    "cheater_conf_matrix = confusion_matrix(true_labels, preds)\n",
    "print(\"Cheater Confusion Matrix:\")\n",
    "print(cheater_conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted model did perform better compared to my \"cheater\" one. The fitted model has less true positives but it has more true negatives, which means the accuracy would be higher for the fitted model. From the confusion matrix, it seems that the model was better when the actual type was indica and worse when predicting when the actual type was sativa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
